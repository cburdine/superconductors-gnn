{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdcc2a44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SuperCon Dataset: Estimating Alloy Phase Diagram\n",
    "---\n",
    "\n",
    "This is a notebook documenting my experiments with using the model to generate alloy phase diagrams:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09edc76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Dependencies:\n",
    "\n",
    "To run this notebook you will need to run `pip3 install <dependency>` for all of the packages listed below. These dependencies should be preinstalled in this project's associated Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e26540",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import ase\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751eb239",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datasets:\n",
    "Running this notebook requires the metal alloy structure data obtained from the Materials project database (in POSCAR file format). This data can be obtained by running the `./scrape_mp_data.py` script. The directory that this data is scraped into must be configured below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a37eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNN_ALLOY_DIR = './data/structures/alignn_alloys'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6633c90",
   "metadata": {},
   "source": [
    "Configure the path to the pretrained ALIGNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0ad7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNN_TC_MODEL_WEIGHTS = './pretrained_model/checkpoint_80.pt'\n",
    "ALIGNN_TC_MODEL_CONFIG = './pretrained_model/config.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824ed7c",
   "metadata": {},
   "source": [
    "This notebook also requires periodic table data, which can be downloaded below:\n",
    "\n",
    "* [Periodic Table of Elements (CSV)](https://gist.github.com/GoodmanSciences/c2dd862cd38f21b0ad36b8f96b4bf1ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdc2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTABLE_CSV = 'data/periodictable/PeriodicTable.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f57fc9",
   "metadata": {},
   "source": [
    "## Parse Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f23afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_formula_regex(elements):\n",
    "    \"\"\" builds a formula parsing regex \"\"\"\n",
    "    mass_re = '([0-9]*\\.[0-9]+|[0-9]+)'\n",
    "    elem_re = '(' + '|'.join(elements) + ')'\n",
    "    return re.compile(elem_re + mass_re)\n",
    "\n",
    "def parse_formula_tokens(formula, regex):\n",
    "    \"\"\" parses a checmical formula consisting of <elem><mass> pairs\"\"\"\n",
    "    tokens = []\n",
    "    for match in regex.finditer(formula):\n",
    "        if match.group(1):\n",
    "            tokens.append((match.group(1), float(match.group(2))))\n",
    "        else:\n",
    "            # assume 1.0 if no mass term:\n",
    "            tokens.append((match.group(1), 1.0))\n",
    "    return tokens\n",
    "\n",
    "def formula_from_tokens(tokens):\n",
    "    \"\"\" Constructs a canonical formula from element tokens\"\"\"\n",
    "    tokens = sorted(tokens)\n",
    "    formula = ''\n",
    "    for elem, n in tokens:\n",
    "        formula += str(elem)\n",
    "        if (n != 1):\n",
    "            formula += str(n)\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee3237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptable_df = pd.read_csv(PTABLE_CSV)\n",
    "elements = list(set(ptable_df['Symbol']))\n",
    "form_re = build_formula_regex(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e920f67",
   "metadata": {},
   "source": [
    "## Use ALIGNN model to predict alloy phase diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6549af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignn.models.alignn import ALIGNN, ALIGNNConfig\n",
    "from jarvis.core.atoms import Atoms\n",
    "from alignn.graphs import Graph\n",
    "import torch\n",
    "\n",
    "def load_model(path):\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    model = ALIGNN(ALIGNNConfig(name=\"alignn\", output_features=1))\n",
    "    model.load_state_dict(torch.load(path, map_location=device)[\"model\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_serve(model, poscar_file):\n",
    "\n",
    "    cutoff = 8.0\n",
    "    max_neighbors = 12\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    atoms = Atoms.from_poscar(poscar_file)\n",
    "    g, lg = Graph.atom_dgl_multigraph(\n",
    "        atoms, cutoff=float(cutoff), max_neighbors=max_neighbors,\n",
    "    )\n",
    "    out_data = (\n",
    "        model([g.to(device), lg.to(device)])\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "        .flatten()\n",
    "        .tolist()\n",
    "    )\n",
    "    return out_data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bedafbb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ALIGNNConfig in module alignn.models.alignn:\n",
      "\n",
      "class ALIGNNConfig(alignn.utils.BaseSettings)\n",
      " |  ALIGNNConfig(_env_file: Union[str, os.PathLike, List[Union[str, os.PathLike]], Tuple[Union[str, os.PathLike], ...], NoneType] = '<object object at 0x7f1d885a9090>', _env_file_encoding: Optional[str] = None, _env_nested_delimiter: Optional[str] = None, _secrets_dir: Union[str, os.PathLike, NoneType] = None, *, name: Literal['alignn'], alignn_layers: int = 4, gcn_layers: int = 4, atom_input_features: int = 92, edge_input_features: int = 80, triplet_input_features: int = 40, embedding_features: int = 64, hidden_features: int = 256, output_features: int = 1, link: Literal['identity', 'log', 'logit'] = 'identity', zero_inflated: bool = False, classification: bool = False, num_classes: int = 2) -> None\n",
      " |  \n",
      " |  Hyperparameter schema for jarvisdgl.models.alignn.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ALIGNNConfig\n",
      " |      alignn.utils.BaseSettings\n",
      " |      pydantic.env_settings.BaseSettings\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'alignn.models.alignn.ALIGNNConfig.Config'>\n",
      " |      Configure model settings behavior.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'alignn_layers': <class 'int'>, 'atom_input_feature...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'alignn.models.alignn.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'alignn_layers': ModelField(name='alignn_layers', type=i...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (_env_file: Union[str, os.PathLike, L...ion...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.env_settings.BaseSettings:\n",
      " |  \n",
      " |  __init__(__pydantic_self__, _env_file: Union[str, os.PathLike, List[Union[str, os.PathLike]], Tuple[Union[str, os.PathLike], ...], NoneType] = '<object object at 0x7f1d885a9090>', _env_file_encoding: Optional[str] = None, _env_nested_delimiter: Optional[str] = None, _secrets_dir: Union[str, os.PathLike, NoneType] = None, **values: Any) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ALIGNNConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alloy_files = glob(os.path.join(ALIGNN_ALLOY_DIR, '*.poscar'))\n",
    "\n",
    "# build alloy map:\n",
    "alloy_map = {}\n",
    "for file in alloy_files:\n",
    "    filename = os.path.split(file)[1]\n",
    "    formula = os.path.splitext(filename)[0]\n",
    "    tokens = parse_formula_tokens(formula, form_re)\n",
    "    elems = tuple(sorted([ t[0] for t in tokens ]))\n",
    "    if elems not in alloy_map:\n",
    "        alloy_map[elems] = []\n",
    "    alloy_map[elems].append((tokens, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04628535",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ALIGNN:\n\tMissing key(s) in state_dict: \"alignn_layers.2.node_update.src_gate.weight\", \"alignn_layers.2.node_update.src_gate.bias\", \"alignn_layers.2.node_update.dst_gate.weight\", \"alignn_layers.2.node_update.dst_gate.bias\", \"alignn_layers.2.node_update.edge_gate.weight\", \"alignn_layers.2.node_update.edge_gate.bias\", \"alignn_layers.2.node_update.bn_edges.weight\", \"alignn_layers.2.node_update.bn_edges.bias\", \"alignn_layers.2.node_update.bn_edges.running_mean\", \"alignn_layers.2.node_update.bn_edges.running_var\", \"alignn_layers.2.node_update.src_update.weight\", \"alignn_layers.2.node_update.src_update.bias\", \"alignn_layers.2.node_update.dst_update.weight\", \"alignn_layers.2.node_update.dst_update.bias\", \"alignn_layers.2.node_update.bn_nodes.weight\", \"alignn_layers.2.node_update.bn_nodes.bias\", \"alignn_layers.2.node_update.bn_nodes.running_mean\", \"alignn_layers.2.node_update.bn_nodes.running_var\", \"alignn_layers.2.edge_update.src_gate.weight\", \"alignn_layers.2.edge_update.src_gate.bias\", \"alignn_layers.2.edge_update.dst_gate.weight\", \"alignn_layers.2.edge_update.dst_gate.bias\", \"alignn_layers.2.edge_update.edge_gate.weight\", \"alignn_layers.2.edge_update.edge_gate.bias\", \"alignn_layers.2.edge_update.bn_edges.weight\", \"alignn_layers.2.edge_update.bn_edges.bias\", \"alignn_layers.2.edge_update.bn_edges.running_mean\", \"alignn_layers.2.edge_update.bn_edges.running_var\", \"alignn_layers.2.edge_update.src_update.weight\", \"alignn_layers.2.edge_update.src_update.bias\", \"alignn_layers.2.edge_update.dst_update.weight\", \"alignn_layers.2.edge_update.dst_update.bias\", \"alignn_layers.2.edge_update.bn_nodes.weight\", \"alignn_layers.2.edge_update.bn_nodes.bias\", \"alignn_layers.2.edge_update.bn_nodes.running_mean\", \"alignn_layers.2.edge_update.bn_nodes.running_var\", \"alignn_layers.3.node_update.src_gate.weight\", \"alignn_layers.3.node_update.src_gate.bias\", \"alignn_layers.3.node_update.dst_gate.weight\", \"alignn_layers.3.node_update.dst_gate.bias\", \"alignn_layers.3.node_update.edge_gate.weight\", \"alignn_layers.3.node_update.edge_gate.bias\", \"alignn_layers.3.node_update.bn_edges.weight\", \"alignn_layers.3.node_update.bn_edges.bias\", \"alignn_layers.3.node_update.bn_edges.running_mean\", \"alignn_layers.3.node_update.bn_edges.running_var\", \"alignn_layers.3.node_update.src_update.weight\", \"alignn_layers.3.node_update.src_update.bias\", \"alignn_layers.3.node_update.dst_update.weight\", \"alignn_layers.3.node_update.dst_update.bias\", \"alignn_layers.3.node_update.bn_nodes.weight\", \"alignn_layers.3.node_update.bn_nodes.bias\", \"alignn_layers.3.node_update.bn_nodes.running_mean\", \"alignn_layers.3.node_update.bn_nodes.running_var\", \"alignn_layers.3.edge_update.src_gate.weight\", \"alignn_layers.3.edge_update.src_gate.bias\", \"alignn_layers.3.edge_update.dst_gate.weight\", \"alignn_layers.3.edge_update.dst_gate.bias\", \"alignn_layers.3.edge_update.edge_gate.weight\", \"alignn_layers.3.edge_update.edge_gate.bias\", \"alignn_layers.3.edge_update.bn_edges.weight\", \"alignn_layers.3.edge_update.bn_edges.bias\", \"alignn_layers.3.edge_update.bn_edges.running_mean\", \"alignn_layers.3.edge_update.bn_edges.running_var\", \"alignn_layers.3.edge_update.src_update.weight\", \"alignn_layers.3.edge_update.src_update.bias\", \"alignn_layers.3.edge_update.dst_update.weight\", \"alignn_layers.3.edge_update.dst_update.bias\", \"alignn_layers.3.edge_update.bn_nodes.weight\", \"alignn_layers.3.edge_update.bn_nodes.bias\", \"alignn_layers.3.edge_update.bn_nodes.running_mean\", \"alignn_layers.3.edge_update.bn_nodes.running_var\", \"gcn_layers.2.src_gate.weight\", \"gcn_layers.2.src_gate.bias\", \"gcn_layers.2.dst_gate.weight\", \"gcn_layers.2.dst_gate.bias\", \"gcn_layers.2.edge_gate.weight\", \"gcn_layers.2.edge_gate.bias\", \"gcn_layers.2.bn_edges.weight\", \"gcn_layers.2.bn_edges.bias\", \"gcn_layers.2.bn_edges.running_mean\", \"gcn_layers.2.bn_edges.running_var\", \"gcn_layers.2.src_update.weight\", \"gcn_layers.2.src_update.bias\", \"gcn_layers.2.dst_update.weight\", \"gcn_layers.2.dst_update.bias\", \"gcn_layers.2.bn_nodes.weight\", \"gcn_layers.2.bn_nodes.bias\", \"gcn_layers.2.bn_nodes.running_mean\", \"gcn_layers.2.bn_nodes.running_var\", \"gcn_layers.3.src_gate.weight\", \"gcn_layers.3.src_gate.bias\", \"gcn_layers.3.dst_gate.weight\", \"gcn_layers.3.dst_gate.bias\", \"gcn_layers.3.edge_gate.weight\", \"gcn_layers.3.edge_gate.bias\", \"gcn_layers.3.bn_edges.weight\", \"gcn_layers.3.bn_edges.bias\", \"gcn_layers.3.bn_edges.running_mean\", \"gcn_layers.3.bn_edges.running_var\", \"gcn_layers.3.src_update.weight\", \"gcn_layers.3.src_update.bias\", \"gcn_layers.3.dst_update.weight\", \"gcn_layers.3.dst_update.bias\", \"gcn_layers.3.bn_nodes.weight\", \"gcn_layers.3.bn_nodes.bias\", \"gcn_layers.3.bn_nodes.running_mean\", \"gcn_layers.3.bn_nodes.running_var\". \n\tsize mismatch for atom_embedding.layer.0.weight: copying a param with shape torch.Size([384, 92]) from checkpoint, the shape in current model is torch.Size([256, 92]).\n\tsize mismatch for atom_embedding.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.0.weight: copying a param with shape torch.Size([384, 64]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for edge_embedding.2.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.0.weight: copying a param with shape torch.Size([384, 64]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for angle_embedding.2.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m BINARY_ALLOY \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m alignn_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mALIGNN_TC_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m alloy_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(BINARY_ALLOY))\n\u001b[1;32m      6\u001b[0m tc_phases \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m ALIGNN(ALIGNNConfig(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignn\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/media/colin/Shared/colin/git/superconductors-gnn/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ALIGNN:\n\tMissing key(s) in state_dict: \"alignn_layers.2.node_update.src_gate.weight\", \"alignn_layers.2.node_update.src_gate.bias\", \"alignn_layers.2.node_update.dst_gate.weight\", \"alignn_layers.2.node_update.dst_gate.bias\", \"alignn_layers.2.node_update.edge_gate.weight\", \"alignn_layers.2.node_update.edge_gate.bias\", \"alignn_layers.2.node_update.bn_edges.weight\", \"alignn_layers.2.node_update.bn_edges.bias\", \"alignn_layers.2.node_update.bn_edges.running_mean\", \"alignn_layers.2.node_update.bn_edges.running_var\", \"alignn_layers.2.node_update.src_update.weight\", \"alignn_layers.2.node_update.src_update.bias\", \"alignn_layers.2.node_update.dst_update.weight\", \"alignn_layers.2.node_update.dst_update.bias\", \"alignn_layers.2.node_update.bn_nodes.weight\", \"alignn_layers.2.node_update.bn_nodes.bias\", \"alignn_layers.2.node_update.bn_nodes.running_mean\", \"alignn_layers.2.node_update.bn_nodes.running_var\", \"alignn_layers.2.edge_update.src_gate.weight\", \"alignn_layers.2.edge_update.src_gate.bias\", \"alignn_layers.2.edge_update.dst_gate.weight\", \"alignn_layers.2.edge_update.dst_gate.bias\", \"alignn_layers.2.edge_update.edge_gate.weight\", \"alignn_layers.2.edge_update.edge_gate.bias\", \"alignn_layers.2.edge_update.bn_edges.weight\", \"alignn_layers.2.edge_update.bn_edges.bias\", \"alignn_layers.2.edge_update.bn_edges.running_mean\", \"alignn_layers.2.edge_update.bn_edges.running_var\", \"alignn_layers.2.edge_update.src_update.weight\", \"alignn_layers.2.edge_update.src_update.bias\", \"alignn_layers.2.edge_update.dst_update.weight\", \"alignn_layers.2.edge_update.dst_update.bias\", \"alignn_layers.2.edge_update.bn_nodes.weight\", \"alignn_layers.2.edge_update.bn_nodes.bias\", \"alignn_layers.2.edge_update.bn_nodes.running_mean\", \"alignn_layers.2.edge_update.bn_nodes.running_var\", \"alignn_layers.3.node_update.src_gate.weight\", \"alignn_layers.3.node_update.src_gate.bias\", \"alignn_layers.3.node_update.dst_gate.weight\", \"alignn_layers.3.node_update.dst_gate.bias\", \"alignn_layers.3.node_update.edge_gate.weight\", \"alignn_layers.3.node_update.edge_gate.bias\", \"alignn_layers.3.node_update.bn_edges.weight\", \"alignn_layers.3.node_update.bn_edges.bias\", \"alignn_layers.3.node_update.bn_edges.running_mean\", \"alignn_layers.3.node_update.bn_edges.running_var\", \"alignn_layers.3.node_update.src_update.weight\", \"alignn_layers.3.node_update.src_update.bias\", \"alignn_layers.3.node_update.dst_update.weight\", \"alignn_layers.3.node_update.dst_update.bias\", \"alignn_layers.3.node_update.bn_nodes.weight\", \"alignn_layers.3.node_update.bn_nodes.bias\", \"alignn_layers.3.node_update.bn_nodes.running_mean\", \"alignn_layers.3.node_update.bn_nodes.running_var\", \"alignn_layers.3.edge_update.src_gate.weight\", \"alignn_layers.3.edge_update.src_gate.bias\", \"alignn_layers.3.edge_update.dst_gate.weight\", \"alignn_layers.3.edge_update.dst_gate.bias\", \"alignn_layers.3.edge_update.edge_gate.weight\", \"alignn_layers.3.edge_update.edge_gate.bias\", \"alignn_layers.3.edge_update.bn_edges.weight\", \"alignn_layers.3.edge_update.bn_edges.bias\", \"alignn_layers.3.edge_update.bn_edges.running_mean\", \"alignn_layers.3.edge_update.bn_edges.running_var\", \"alignn_layers.3.edge_update.src_update.weight\", \"alignn_layers.3.edge_update.src_update.bias\", \"alignn_layers.3.edge_update.dst_update.weight\", \"alignn_layers.3.edge_update.dst_update.bias\", \"alignn_layers.3.edge_update.bn_nodes.weight\", \"alignn_layers.3.edge_update.bn_nodes.bias\", \"alignn_layers.3.edge_update.bn_nodes.running_mean\", \"alignn_layers.3.edge_update.bn_nodes.running_var\", \"gcn_layers.2.src_gate.weight\", \"gcn_layers.2.src_gate.bias\", \"gcn_layers.2.dst_gate.weight\", \"gcn_layers.2.dst_gate.bias\", \"gcn_layers.2.edge_gate.weight\", \"gcn_layers.2.edge_gate.bias\", \"gcn_layers.2.bn_edges.weight\", \"gcn_layers.2.bn_edges.bias\", \"gcn_layers.2.bn_edges.running_mean\", \"gcn_layers.2.bn_edges.running_var\", \"gcn_layers.2.src_update.weight\", \"gcn_layers.2.src_update.bias\", \"gcn_layers.2.dst_update.weight\", \"gcn_layers.2.dst_update.bias\", \"gcn_layers.2.bn_nodes.weight\", \"gcn_layers.2.bn_nodes.bias\", \"gcn_layers.2.bn_nodes.running_mean\", \"gcn_layers.2.bn_nodes.running_var\", \"gcn_layers.3.src_gate.weight\", \"gcn_layers.3.src_gate.bias\", \"gcn_layers.3.dst_gate.weight\", \"gcn_layers.3.dst_gate.bias\", \"gcn_layers.3.edge_gate.weight\", \"gcn_layers.3.edge_gate.bias\", \"gcn_layers.3.bn_edges.weight\", \"gcn_layers.3.bn_edges.bias\", \"gcn_layers.3.bn_edges.running_mean\", \"gcn_layers.3.bn_edges.running_var\", \"gcn_layers.3.src_update.weight\", \"gcn_layers.3.src_update.bias\", \"gcn_layers.3.dst_update.weight\", \"gcn_layers.3.dst_update.bias\", \"gcn_layers.3.bn_nodes.weight\", \"gcn_layers.3.bn_nodes.bias\", \"gcn_layers.3.bn_nodes.running_mean\", \"gcn_layers.3.bn_nodes.running_var\". \n\tsize mismatch for atom_embedding.layer.0.weight: copying a param with shape torch.Size([384, 92]) from checkpoint, the shape in current model is torch.Size([256, 92]).\n\tsize mismatch for atom_embedding.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for atom_embedding.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.0.weight: copying a param with shape torch.Size([384, 64]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for edge_embedding.2.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for edge_embedding.2.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.0.weight: copying a param with shape torch.Size([384, 64]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for angle_embedding.2.layer.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for angle_embedding.2.layer.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.node_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.node_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.0.edge_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.0.edge_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.node_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.node_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for alignn_layers.1.edge_update.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for alignn_layers.1.edge_update.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.0.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.0.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.src_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.src_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.dst_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.dst_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.edge_gate.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.edge_gate.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_edges.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.src_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.src_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.dst_update.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for gcn_layers.1.dst_update.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for gcn_layers.1.bn_nodes.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 256])."
     ]
    }
   ],
   "source": [
    "BINARY_ALLOY = ('Nb', 'Ge')\n",
    "\n",
    "alignn_model = load_model(ALIGNN_TC_MODEL_PATH)\n",
    "\n",
    "alloy_key = tuple(sorted(BINARY_ALLOY))\n",
    "tc_phases = []\n",
    "\n",
    "# query model with intermediate compounds/alloys:\n",
    "for tokens, alloy_file in alloy_map[alloy_key]:\n",
    "    pred_tc = model_serve(alignn_model, alloy_file)\n",
    "    tc_phases.append((tokens, pred_tc))\n",
    "    print(alloy_file, pred_tc)\n",
    "\n",
    "# also query elemental \"endpoints\" of the phase space:\n",
    "for elem in BINARY_ALLOY:\n",
    "    elems_key = (elem,)\n",
    "    if elems_key in alloy_map:\n",
    "        tokens, alloy_file = alloy_map[elems_key][-1]\n",
    "        pred_tc = model_serve(alignn_model, alloy_file)\n",
    "        tc_phases.append((tokens, pred_tc))\n",
    "        print(alloy_file, pred_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3584f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tc_phases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tcs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m vecs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (tokens, tc) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtc_phases\u001b[49m):\n\u001b[1;32m      5\u001b[0m     composition \u001b[38;5;241m=\u001b[39m { e : n \u001b[38;5;28;01mfor\u001b[39;00m (e,n) \u001b[38;5;129;01min\u001b[39;00m tokens }\n\u001b[1;32m      6\u001b[0m     vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ \n\u001b[1;32m      7\u001b[0m         composition[e] \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m composition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m BINARY_ALLOY \n\u001b[1;32m      9\u001b[0m     ]) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mlist\u001b[39m(composition\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tc_phases' is not defined"
     ]
    }
   ],
   "source": [
    "tcs = []\n",
    "vecs = []\n",
    "\n",
    "for (tokens, tc) in sorted(tc_phases):\n",
    "    composition = { e : n for (e,n) in tokens }\n",
    "    vector = np.array([ \n",
    "        composition[e] if e in composition else 0.0\n",
    "        for e in BINARY_ALLOY \n",
    "    ]) / np.sum(list(composition.values()))\n",
    "    \n",
    "    tcs.append(tc)\n",
    "    vecs.append(vector)\n",
    "\n",
    "x0 = np.array([[0.],[1.]])\n",
    "x = np.array([ v @ x0 for v in vecs ])\n",
    "\n",
    "plt.ylabel(r'$T_c$', fontsize=12)\n",
    "plt.xlabel(r'$x$', fontsize=12)\n",
    "plt.title(r'Nb$_x$Ge$_{1-x}$ $T_c$ under doping')\n",
    "plt.fill_between(x.flatten(), tcs)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
